{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edbbebd4",
   "metadata": {},
   "source": [
    "<H2>Download Audio Data from YouTube Video Clip URLs<H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b0880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pytube import YouTube\n",
    "\n",
    "def download_youtube_audio(url, output_dir='youtube_downloads'):\n",
    "    '''\n",
    "    This function is for downloading audio content from youtube video clips.\n",
    "    These are saved as wav files in the local directory. This code was guided by\n",
    "    work completed in Ficano. (2023)\n",
    "    '''\n",
    "    try:\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # using youtube library to instantiate youtube instance\n",
    "        yt = YouTube(url)\n",
    "\n",
    "        audio_stream = yt.streams.filter(only_audio=True, \n",
    "                                               file_extension='webm').first()\n",
    "\n",
    "        # save the best quality audio_stream to local storage\n",
    "        audio_stream.download(output_path=output_dir)\n",
    "\n",
    "        # this project uses wav file extensions, ensure this is the extension used\n",
    "        original_file_path = os.path.join(output_dir, audio_stream.default_filename)\n",
    "        wav_file_path = os.path.join(output_dir, f\"{you_tube.title}.wav\")\n",
    "        os.rename(original_file_path, wav_file_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"something went wrong: {e}\")\n",
    "\n",
    "\n",
    "youtube_url = input(\"YouTube URL to download: \")\n",
    "download_youtube_audio(youtube_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca793f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53ede0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c50b098f",
   "metadata": {},
   "source": [
    "<H2>Word Segmentation<H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisperx\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "'''\n",
    "This script creates new wav files containing single word utteranes. These words are passed in via a configuration \n",
    "object that contains a list of targets words to extract from the full youtube audio clips.\n",
    "This sript also creates a corresponding text file with the transcription and the start and end times for each word in the file.\n",
    "The implmentation of this script was guided strongly by the work completed in:\n",
    "Bain (2023) which includes easy to follow instructions on getting WhisperX up and running\n",
    "'''\n",
    "\n",
    "\n",
    "def preprocess_audio(audio_path, desired_sample_rate=16000):\n",
    "\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "    # resample to match the wav2vec frame rate\n",
    "    audio = audio.set_frame_rate(desired_sample_rate)\n",
    "    return audio\n",
    "\n",
    "def transcribe_and_save_timestamps(audio_path, output_path, model, device, batch_size):\n",
    "    audio = whisperx.load_audio(audio_path)\n",
    "    result = model.transcribe(audio, language=\"en\", batch_size=batch_size)\n",
    "\n",
    "    whisperx_model, metadata = whisperx.load_align_model(language_code=result[\"language\"], \n",
    "                                                  device=device)\n",
    "    aligned_result = whisperx.align(result[\"segments\"], \n",
    "                                    whisperx_model, \n",
    "                                    metadata, \n",
    "                                    audio, \n",
    "                                    device=device)\n",
    "\n",
    "    with open(output_path, 'w') as trans_file:\n",
    "        for segment in aligned_result[\"segments\"]:\n",
    "            for words in segment[\"words\"]:\n",
    "                # in some cases, there is no start and end time, need to handle this through some if statements\n",
    "                if 'start' in words and 'end' in words and 'score' in words:\n",
    "                    trans_file.write(f\"{words['word']} {words['start']} {words['end']} {words['score']}\\n\")\n",
    "                else:\n",
    "                    trans_file.write(f\"{words['word']} - - -\\n\")  # hitting some missing data e.g. years 2020\n",
    "\n",
    "\n",
    "def get_top_3_timestamps_from_file(timestamp_file, target_word):\n",
    "    scores = []\n",
    "\n",
    "    with open(timestamp_file, 'r') as file:\n",
    "        for line in file:\n",
    "            word, start, end, score = line.strip().split()\n",
    "            if word.lower() == target_word.lower():\n",
    "                scores.append((float(start), float(end), float(score)))\n",
    "\n",
    "    # sort the list by probabilty score in descending order and return the top 3\n",
    "    scores.sort(key=lambda x: x[2], reverse=True)\n",
    "    return scores[:3]\n",
    "\n",
    "\n",
    "config = [\n",
    "    {\n",
    "        \"desired_accent\": \"Irish English\",\n",
    "        \"feature\": \"words\",\n",
    "        \"base_destination_dir\": \"/home/garrett/Workspace/msc/speech_seg/london/features/words/\",\n",
    "        \"words\": [\"beat\", \"feet\", \"seat\", \"meet\", \"greet\", \"heat\", \"neat\", \"sweet\", \"treat\", \"complete\", \"bit\", \"sit\",\n",
    "                             \"kit\", \"lit\", \"fit\", \"hit\", \"tip\", \"man\", \"pan\", \"can\", \"fan\", \"car\", \"ban\", \"ran\", \"van\", \"plan\", \"hand\", \"land\",\n",
    "                             \"not\", \"stand\", \"boat\", \"coat\", \"float\", \"vote\", \"note\", \"throat\", \"wrote\", \"quote\", \"gloat\", \"roam\",\n",
    "                             \"but\", \"book\", \"word\", \"hate\", \"bite\", \"light\", \"might\", \"fight\", \"night\", \"right\", \"sight\",\n",
    "                             \"tight\", \"write\", \"bright\", \"height\", \"delight\", \"about\", \"shout\", \"doubt\", \"scout\", \"proud\", \"loud\",\n",
    "                             \"crowd\", \"mouth\", \"south\", \"boot\", \"shoot\", \"fruit\", \"group\", \"suit\", \"mute\", \"root\",\n",
    "                             \"thought\", \"caught\", \"taught\", \"bought\", \"fought\", \"road\", \"load\", \"mode\", \"remote\", \"hope\", \n",
    "                 \"rope\", \"slope\", \"loaf\", \"goat\", \"host\", \"most\", \"roast\", \"toast\"]\n",
    "  \n",
    "        \n",
    "    },\n",
    "\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "# these are not typical asr settings, had to pass these in to overcome a bug in whisperx\n",
    "# the model would not let me continue uness temperature setting was passed in \n",
    "# please see bug: https://github.com/SYSTRAN/faster-whisper/issues/455\n",
    "asr_options = {\n",
    "    \"repetition_penalty\": 1, \n",
    "    \"prompt_reset_on_temperature\": 0.5,\n",
    "    \"no_repeat_ngram_size\": 0\n",
    "}\n",
    "\n",
    "device = \"cpu\"\n",
    "batch_size = 8\n",
    "model = whisperx.load_model(\"small.en\", device, compute_type=\"float32\", asr_options=asr_options)\n",
    "\n",
    "audio_data_dir = \"/home/garrett/Workspace/msc/speech_seg/london/audio_data/\"\n",
    "features_dir = \"/home/garrett/Workspace/msc/speech_seg/london/features/\"\n",
    "audio_data_cleaned_dir = \"/home/garrett/Workspace/msc/speech_seg/london/audio_data_cleaned/\"\n",
    "\n",
    "# transcribe audio file just once and save timestamps, if transcribed file already exists, use it\n",
    "for filename in os.listdir(audio_data_dir):\n",
    "    if filename.endswith('.wav'):\n",
    "        audio_path = os.path.join(audio_data_dir, filename)\n",
    "        destination_wav_path = os.path.join(audio_data_cleaned_dir, filename)\n",
    "        timestamp_file = os.path.join(audio_data_cleaned_dir, f\"{filename}.txt\")\n",
    "        \n",
    "        if not os.path.exists(timestamp_file):\n",
    "            print(f\"Transcribing AUDIO: {audio_path}\")\n",
    "            audio = preprocess_audio(audio_path)\n",
    "            audio.export(destination_wav_path, format='wav')\n",
    "            transcribe_and_save_timestamps(destination_wav_path, timestamp_file, model, device, batch_size)\n",
    "        else:\n",
    "            print(f\"Timestamps already exist for {filename}, skipping transcription.\")\n",
    "            \n",
    "            \n",
    "\n",
    "# iterate over the features\n",
    "for feature_config in config:\n",
    "    feature = feature_config[\"feature\"]\n",
    "    words = feature_config[\"words\"]\n",
    "\n",
    "    for filename in os.listdir(audio_data_dir):\n",
    "        if filename.endswith('.wav'):\n",
    "            print(f\"processing {feature} for audiofile: {filename}\")\n",
    "            timestamp_file = os.path.join(audio_data_cleaned_dir, f\"{filename}.txt\")\n",
    "\n",
    "            for word in words:\n",
    "                top_timestamps = get_top_3_timestamps_from_file(timestamp_file, word)\n",
    "                for index, (start_sec, end_sec, score) in enumerate(top_timestamps):\n",
    "                    start_ms = int(start_sec * 1000)\n",
    "                    end_ms = int(end_sec * 1000)\n",
    "                    \n",
    "                    audio = AudioSegment.from_file(os.path.join(audio_data_cleaned_dir, filename))\n",
    "                    word_audio = audio[start_ms:end_ms]\n",
    "\n",
    "                    save_dir = os.path.join(features_dir, feature, word)\n",
    "                    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "                    file_name_without_extension = filename[:-4]\n",
    "                    file_dir = os.path.join(save_dir, file_name_without_extension)\n",
    "                    os.makedirs(file_dir, exist_ok=True)  \n",
    "\n",
    "                    save_path = os.path.join(file_dir, f\"{word}_{index + 1}.wav\") #index to filename\n",
    "                    word_audio.export(save_path, format='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9cbb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5e3bbab",
   "metadata": {},
   "source": [
    "<H2>Phoneme Segmentation<H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d79bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import eng_to_ipa as ipa\n",
    "import time\n",
    "import scipy.io.wavfile\n",
    "import shutil\n",
    "\n",
    "'''\n",
    "This script creates a new wav file for every phoneme detected in the word files. \n",
    "It iterates over the word files that exists in a given base directory (provided via the cofig var).\n",
    "For each word, extract the phonemes using the Wav2Vec model and save these phonemes as a new wav file.\n",
    "The work completed in this script was guided by the work seen in :\n",
    "Xu (2021) and code samples provided in Baevski (2021).\n",
    "'''\n",
    "\n",
    "\n",
    "# when rerunning this script, some of residual files were causing issues\n",
    "# this funciton is used to clean up files for a fresh execution\n",
    "def clear_directory(directory):\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f'error deleting {file_path} : {e}')\n",
    "\n",
    "def save_phonemes_as_audio(audio, phoneme_list, output_dir, sr):\n",
    "\n",
    "    audio_segment_data = None\n",
    "    audio_int16 = None\n",
    "    start_time_ms= None\n",
    "    end_time_ms = None\n",
    "    start_sample = None\n",
    "    end_sample = None\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "\n",
    "\n",
    "    for item in phoneme_list:\n",
    "        phoneme = item['phoneme']\n",
    "        if phoneme.strip() == '':\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            start_ms = int(float(item['start_time']))\n",
    "            end_ms = int(float(item['end_time']))\n",
    "        except Exception as e:\n",
    "            print(\"error casting start and end times: \" + str(e))\n",
    "        \n",
    "        start_sample = int(start_ms * sr / 1000)\n",
    "        end_sample = int(end_ms * sr / 1000)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            audio_segment_data = audio[start_sample:end_sample]\n",
    "        except Exception as e:\n",
    "            print(f\"error creating audio segnment data:  {e}\")\n",
    "            \n",
    "        # adding 2 second buffer at start and end of audio, this seems to \n",
    "        # help in the performance of word and phoneme segmentation\n",
    "        silence_duration = 2  # seconds\n",
    "        silence_samples = int(silence_duration * sr)\n",
    "        silence = np.zeros(silence_samples)\n",
    "        audio_segment_data_silenced = np.concatenate((silence, audio_segment_data, silence))\n",
    "        \n",
    "\n",
    "        filename = os.path.join(output_dir, f\"{phoneme}_{start_ms:.2f}_{end_ms:.2f}.wav\")\n",
    "        try:\n",
    "            scipy.io.wavfile.write(filename, sr, audio_segment_data_silenced)\n",
    "        except Exception as e:\n",
    "            print(f\"Error exporting audio for phoneme '{phoneme}': {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def process_audio(file_path, word, model, processor, phoneme_directory, chunk_duration=1.0):\n",
    "    try:\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        audio = None\n",
    "        input_values = None\n",
    "        predicted_ids = None\n",
    "        transcription = None\n",
    "        \n",
    "        audio, orig_sampling_rate = librosa.load(file_path, sr=16000)\n",
    "\n",
    "        \n",
    "        audio_silenced = None\n",
    "        silence_duration = 2\n",
    "        silence_samples = int(silence_duration * orig_sampling_rate)\n",
    "        silence = np.zeros(silence_samples)\n",
    "        audio_silenced = np.concatenate((silence, audio, silence))\n",
    "        \n",
    "        total_duration_seconds = len(audio) / orig_sampling_rate\n",
    "\n",
    "        input_values = processor(audio_silenced, sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_values).logits\n",
    "\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        predicted_ids = torch.argmax(probabilities, dim=-1)\n",
    "        confidence_scores = probabilities.max(dim=-1).values\n",
    "        \n",
    "        transcription = processor.batch_decode(predicted_ids)[0]\n",
    "        \n",
    "        print(transcription)\n",
    "        \n",
    "        duration_per_token = total_duration_seconds * 1000 / len(transcription)  # milliseconds\n",
    "        \n",
    "        phoneme_list = []\n",
    "        phoneme_confidences = []\n",
    "        for i, phoneme in enumerate(transcription):\n",
    "            start_time = i * duration_per_token\n",
    "            end_time = (i + 1) * duration_per_token\n",
    "            phoneme_confidence = confidence_scores[0, i].item()  # confidence scor for each phoneme\n",
    "            phoneme_confidences.append(phoneme_confidence)\n",
    "            phoneme_list.append({\n",
    "                'phoneme': phoneme,\n",
    "                'start_time': f\"{start_time:.2f}\",\n",
    "                'end_time': f\"{end_time:.2f}\",\n",
    "                'probability': phoneme_confidence\n",
    "            })\n",
    "        \n",
    "        for phoneme in phoneme_list:\n",
    "            print(f\"Phoneme: {phoneme['phoneme']}, Probability: {phoneme['probability']:.4f}\")\n",
    "\n",
    "        \n",
    "        output_dir = os.path.join(os.path.dirname(file_path), phoneme_directory)\n",
    "        print(output_dir)\n",
    "        # clear output directory before processing to avoid duplicates\n",
    "        if os.path.exists(output_dir):\n",
    "            clear_directory(output_dir)\n",
    "            \n",
    "        save_phonemes_as_audio(audio, phoneme_list, output_dir, orig_sampling_rate)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        pass\n",
    "\n",
    "def main():\n",
    "\n",
    "    \n",
    "    config = [\n",
    "    {\n",
    "        \"desired_accent\": \"Irish English\",\n",
    "        \"feature\": \"words\",\n",
    "        \"base_destination_dir\": \"/home/garrett/Workspace/msc/speech_seg/london/features/words/\",\n",
    "        \"words\": [\"beat\", \"feet\", \"seat\", \"meet\", \"greet\", \"heat\", \"neat\", \"sweet\", \"treat\", \"complete\", \"bit\", \"sit\",\n",
    "                             \"kit\", \"lit\", \"fit\", \"hit\", \"tip\", \"man\", \"pan\", \"can\", \"fan\", \"car\", \"ban\", \"ran\", \"van\", \"plan\", \"hand\", \"land\",\n",
    "                             \"not\", \"stand\", \"boat\", \"coat\", \"float\", \"vote\", \"note\", \"throat\", \"wrote\", \"quote\", \"gloat\", \"roam\",\n",
    "                             \"but\", \"book\", \"word\", \"hate\", \"bite\", \"light\", \"might\", \"fight\", \"night\", \"right\", \"sight\",\n",
    "                             \"tight\", \"write\", \"bright\", \"height\", \"delight\", \"about\", \"shout\", \"doubt\", \"scout\", \"proud\", \"loud\",\n",
    "                             \"crowd\", \"mouth\", \"south\", \"boot\", \"shoot\", \"fruit\", \"group\", \"suit\", \"mute\", \"root\",\n",
    "                             \"thought\", \"caught\", \"taught\", \"bought\", \"fought\", \"road\", \"load\", \"mode\", \"remote\", \"hope\", \n",
    "                 \"rope\", \"slope\", \"loaf\", \"goat\", \"host\", \"most\", \"roast\", \"toast\"]\n",
    "  \n",
    "        \n",
    "    },\n",
    "\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # this is the hugging face model used for phonenem seg\n",
    "        model_name = \"facebook/wav2vec2-lv-60-espeak-cv-ft\"\n",
    "        model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "        processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "    except Exception as e:\n",
    "        print(f\"error creating model: {e}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    try:\n",
    "        # iterae of the word wav files in each base directory and extract the relevant phonemes\n",
    "        # from these word files. Each phoneme is saved as a new wav file \n",
    "        for accent_config in config:\n",
    "            base_dir = accent_config[\"base_destination_dir\"]\n",
    "            \n",
    "            for word in accent_config[\"words\"]:\n",
    "                word_dir = os.path.join(base_dir, word)\n",
    "                if os.path.exists(word_dir):\n",
    "                    individual_dirs = [d for d in os.listdir(word_dir) if os.path.isdir(os.path.join(word_dir, d))]\n",
    "                    for individual_dir in individual_dirs:\n",
    "                        individual_dir_path = os.path.join(word_dir, individual_dir)\n",
    "                        word_files = [f for f in os.listdir(individual_dir_path) if f.endswith(\".wav\")]\n",
    "                        for word_file in word_files:\n",
    "                            file_path = os.path.join(individual_dir_path, word_file)\n",
    "                            phoneme_directory = \"phonemes_\" + str(word_file[:-4])\n",
    "                            if os.path.exists(file_path):\n",
    "                                process_audio(file_path, word, model, processor, phoneme_directory)\n",
    "                                time.sleep(3)\n",
    "            \n",
    "            \n",
    "            \n",
    "                            \n",
    "    except Exception as e:\n",
    "        print(f\"error getting words for processing: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a678a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213d892b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e8f890e",
   "metadata": {},
   "source": [
    "<H2>Pitch Analysis<H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1e1cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "def calc_mean_pitch(y, sr):\n",
    "    '''\n",
    "    get the mean pitch for an audio file containg utterance\n",
    "    Used the following documentaiton for guidance: \n",
    "    pavlos163 (2017)\n",
    "    '''\n",
    "    pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "    pitch = []\n",
    "    for t in range(pitches.shape[1]):\n",
    "        index = magnitudes[:, t].argmax()\n",
    "        pitch.append(pitches[index, t])\n",
    "\n",
    "    pitch = np.array(pitch)\n",
    "    pitch = pitch[pitch > 0]\n",
    "    mean_pitch = np.mean(pitch)\n",
    "    return mean_pitch\n",
    "\n",
    "def get_pitch(directory):\n",
    "    mean_pitch_vals = []\n",
    "    #iterate of the file directory and ge the mean pitch values\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.wav'):\n",
    "            y, sr = librosa.load(os.path.join(directory, filename))\n",
    "            pitch = calc_mean_pitch(y, sr)\n",
    "            mean_pitch_vals.append((filename, pitch))\n",
    "\n",
    "    return mean_pitch_vals\n",
    "    \n",
    "    \n",
    "def perform_mann_whitney_and_permutation_test(data_1, data_2, perm_count=1000):\n",
    "    '''\n",
    "    perform mann whintey permutation testing , this code was guided by (same as pitch):\n",
    "    \n",
    "    Cicek (2022)\n",
    "    and\n",
    "    Deak (2022)\n",
    "    \n",
    "    this function assumes the data is balanced for the 2 accent data sets.\n",
    "    '''\n",
    "    original_stat, original_p_value = stats.mannwhitneyu(data_1, data_2, alternative='two-sided')\n",
    "\n",
    "    combined_data = np.concatenate([data_1, data_2])\n",
    "    n = len(data_1)\n",
    "\n",
    "    perm_stats = np.zeros(perm_count)\n",
    "    for i in range(perm_count):\n",
    "        np.random.shuffle(combined_data)\n",
    "        perm_stat, _ = stats.mannwhitneyu(combined_data[:n], combined_data[n:], alternative='two-sided')\n",
    "        perm_stats[i] = perm_stat\n",
    "\n",
    "    p_value_perm = np.mean(perm_stats >= original_stat)\n",
    "\n",
    "    return original_stat, original_p_value, p_value_perm\n",
    "    \n",
    "\n",
    "utterances = ['a', 'i', 'o', 'u']\n",
    "\n",
    "base_directories = {\n",
    "    'Dublin': '/home/garrett/Workspace/msc/speech_seg/dublin/features/Monophthongs/',\n",
    "    'London': '/home/garrett/Workspace/msc/speech_seg/london/features/Monophthongs/',\n",
    "    'Liverpool': '/home/garrett/Workspace/msc/speech_seg/liverpool/features/Monophthongs/'\n",
    "}\n",
    "\n",
    "for utterance in utterances:\n",
    "    pitch_values = {}\n",
    "    min_count = float('inf')\n",
    "\n",
    "    for accent, base_dir in base_directories.items():\n",
    "        directory = os.path.join(base_dir, utterance)\n",
    "        pitches = get_pitch(directory)\n",
    "        pitch_values[accent] = pd.Series([pitch for _, pitch in pitches], name=accent)\n",
    "        min_count = min(min_count, len(pitch_values[accent]))\n",
    "\n",
    "    for accent in pitch_values:\n",
    "        print(f\"{accent} descriptive stats for '{utterance}':\")\n",
    "        print(pitch_values[accent].describe())\n",
    "        print(\" \")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for accent, values in pitch_values.items():\n",
    "        sns.histplot(values, label=accent, kde=True)\n",
    "    plt.title(f\"pitch distribution on '{utterance}'\")\n",
    "    plt.xlabel(\"pitch in Hz\")\n",
    "    plt.ylabel(\"freq\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=list(pitch_values.values()), palette=[\"blue\", \"red\", \"yellow\"])\n",
    "    plt.xticks(range(len(pitch_values)), pitch_values.keys())\n",
    "    plt.title(f\"pitch comparison on '{utterance}'\")\n",
    "    plt.ylabel(\"pitch in Hz\")\n",
    "    plt.show()\n",
    "\n",
    "    for accent, values in pitch_values.items():\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        stats.probplot(values, dist=\"norm\", plot=plt)\n",
    "        plt.title(f\"Q-Q plot on {accent} pitch data ('{utterance}')\")\n",
    "        plt.show()\n",
    "\n",
    "    # shapiro-wilk normality testing\n",
    "    print(\" \")\n",
    "    print(\"shapiro-wilk test for normality\")\n",
    "    for accent in pitch_values:\n",
    "        _, p_value_normality = stats.shapiro(pitch_values[accent])\n",
    "        print(f\"{accent}: p-value = {p_value_normality}\")\n",
    "    print(\" \")\n",
    "\n",
    "    # levene's variance testing\n",
    "    _, p_value_levene = stats.levene(*pitch_values.values())\n",
    "    print(\" \")\n",
    "    print(\"levene's test for homogeneity of variance\")\n",
    "    print(f\"p-value = {p_value_levene}\")\n",
    "    print(\" \")\n",
    "\n",
    "    accents = list(pitch_values.keys())\n",
    "    for i in range(len(accents)):\n",
    "        for x in range(i + 1, len(accents)):\n",
    "            perform_mann_whitney_and_permutation_test(pitch_values[accents[i]], pitch_values[accents[x]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cd77ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a6cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fb9a2da",
   "metadata": {},
   "source": [
    "<H2>Duration analysis<H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d48a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "# if you are not running this script in a notebook environment, you may need to import\n",
    "# perform_mann_whitney_and_permutation_test from the pitch script.\n",
    "\n",
    "def calculate_durations(directory, accent, sound):\n",
    "    durations = []\n",
    "    print(\"DIRECTORY: \" + str(directory))\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.wav'):\n",
    "            print(\"FILENAME: \" + str(filename))\n",
    "            audio_path = os.path.join(directory, filename)\n",
    "            audio, sr = librosa.load(audio_path, sr=None)\n",
    "            duration = librosa.get_duration(audio, sr=sr) - 4\n",
    "            durations.append({'accent': accent, 'duration': duration, 'dound': sound})\n",
    "            print(durations)\n",
    "    return durations\n",
    "\n",
    "\n",
    "\n",
    "def analyse_durations(dataframe, sounds, base_directories):\n",
    "    for utterance in utterances:\n",
    "        current_utterance_data = dataframe[dataframe['utterance'] == utterance]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(data=current_utterance_data, x='duration', hue='accent', element='step', \n",
    "                     stat='density', common_norm=False)\n",
    "        plt.title(f\"distribution of '{utterance}' sound durations\")\n",
    "        plt.xlabel('duration (seconds)')\n",
    "        plt.ylabel('density')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(data=current_utterance_data, x='accent', y='duration', palette=[\"blue\", \"red\", \"yellow\"])\n",
    "        plt.title(f\"boxplot of '{utterance}' utterance durations\")\n",
    "        plt.xlabel('accent')\n",
    "        plt.ylabel('duration in seconds')\n",
    "        plt.show()\n",
    "\n",
    "        accents = list(base_directories.keys())\n",
    "        for i in range(len(accents)):\n",
    "            for x in range(i+1, len(accents)):\n",
    "                data_1 = current_sound_data[current_sound_data['accent'] == accents[i]]['duration'].to_numpy()\n",
    "                data_2 = current_sound_data[current_sound_data['accent'] == accents[x]]['duration'].to_numpy()\n",
    "                original_stat, original_p_value, p_value_perm = perform_mann_whitney_and_permutation_test(data_1, \n",
    "                                                                                                          data_2)\n",
    "                print(f\"mann whitney U test between {accents[i]} and {accents[x]} for utterance '{utterance}': U={original_stat}, Original p-value={original_p_value}, Permutation p-value={p_value_perm}\")\n",
    "\n",
    "duration_data = pd.DataFrame()\n",
    "\n",
    "for utterance in utterances:\n",
    "    for accent, base_dir in base_directories.items():\n",
    "        directory = os.path.join(base_dir, utterance)\n",
    "        duration_data = pd.concat([duration_data, \n",
    "                                       pd.DataFrame(calculate_durations(directory, accent, utterance))])\n",
    "\n",
    "analyse_durations(duration_data, utterances, base_directories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66c03cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89367e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6758720a",
   "metadata": {},
   "source": [
    "<H2>Formant analysis<H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce686324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "# if you are not running this script in a notebook environment, you may need to import\n",
    "# perform_mann_whitney_and_permutation_test from the pitch script.\n",
    "\n",
    "def load_audio(file_path):\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    return audio, sr\n",
    "\n",
    "def trim_silence(audio, sr):\n",
    "    '''\n",
    "    A silence buffer was applied to improve the performance the NN model.\n",
    "    This is a quick access function added to trim the buffer off when needed. \n",
    "    '''\n",
    "    return audio[2*sr:-2*sr]\n",
    "\n",
    "def get_formants(audio, sr):\n",
    "    '''\n",
    "    this code detects the top 3 freqency peaks which represents the formants.\n",
    "    This code was guided by the work completed in:\n",
    "    Rathee, A. (2020)\n",
    "    and \n",
    "    rayryeng. (2020)\n",
    "    and\n",
    "    McFee et al. (2015) \n",
    "    '''\n",
    "    magnitude_spec = np.abs(librosa.stft(audio))\n",
    "    mean_spectrum = np.mean(magnitude_spec, axis=1)\n",
    "    peaks = librosa.util.peak_pick(mean_spectrum, pre_max=3, \n",
    "                                   post_max=3, pre_avg=3, \n",
    "                                   post_avg=5, delta=0.5, wait=10)\n",
    "    formants = sorted(peaks[:3])\n",
    "    # convert formant frequencies (top 3 obtained in previous line of code, \n",
    "    # to the matching frequency values in hertz which is what is needed for the analysis\n",
    "    formants_hz = librosa.core.fft_frequencies(sr=sr)[formants]\n",
    "    return formants_hz if len(formants_hz) == 3 else [0, 0, 0]\n",
    "\n",
    "\n",
    "def process_directory(directory):\n",
    "    formant_data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.wav'):\n",
    "            audio, sr = load_audio(os.path.join(directory, filename))\n",
    "            audio = trim_silence(audio, sr)\n",
    "            formants = get_formants(audio, sr)\n",
    "            formant_data.append([filename] + list(formants))\n",
    "    return pd.DataFrame(formant_data, columns=['Filename', 'F1', 'F2', 'F3'])\n",
    "\n",
    "\n",
    "# iterate through the utterances\n",
    "for utterance in utterances:\n",
    "    print(f\"utternance: '{utterance}'\")\n",
    "    accent_data = {}\n",
    "\n",
    "\n",
    "    for accent, base_dir in base_directories.items():\n",
    "        directory = os.path.join(base_dir, utterance)\n",
    "        data = process_directory(directory)\n",
    "        accent_data[accent] = data\n",
    "\n",
    "    # balance the data sets, this is only needed if the data is imbalanced, which it was for this proejct\n",
    "    min_count = min(len(accent_data[accent]) for accent in accent_data)\n",
    "    for accent in accent_data:\n",
    "        if len(accent_data[accent]) > min_count:\n",
    "            accent_data[accent] = accent_data[accent].sample(min_count, random_state=8)\n",
    "\n",
    "        print(f\"{accent} descriptive stats for utterance '{utterance}':\")\n",
    "        print(accent_data[accent].describe().drop('count'))\n",
    "        print(\" \")\n",
    "\n",
    "    # get the mann whitney U test results and execute and \n",
    "    # execute a permutation test for each formant also\n",
    "    print(\"mann whitney U and perm test results\")\n",
    "    for formant in ['F1', 'F2', 'F3']:\n",
    "        for accent_1 in base_directories:\n",
    "            for accent_2 in base_directories:\n",
    "                if accent_1 < accent_2:  # there was an issue with duplication on the results\n",
    "                    formant_data_1 = accent_data[accent_1][formant]\n",
    "                    formant_data_2 = accent_data[accent_2][formant]\n",
    "                    stat, p_value, p_value_perm = perform_mann_whitney_and_permutation_test(formant_data_1, \n",
    "                                                                                            formant_data_2)\n",
    "                    # for each formant, print out the results so these can be copied into a table for report\n",
    "                    print(f\"{formant} between {accent_1} and {accent_2} for '{utterance}': U={stat}, p-value={p_value}, Permutation p-value={p_value_perm}\")\n",
    "    print(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8fa90f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c987fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a2873f5",
   "metadata": {},
   "source": [
    "<H2>Intensity Analysis<H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554f7a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "# if you are not running this script in a notebook environment, you may need to import\n",
    "# perform_mann_whitney_and_permutation_test from the pitch script.\n",
    "\n",
    "def calculate_mean_intensity(file_path):\n",
    "    '''\n",
    "    this function is responsible for calculating the average loudness of the passed in file path.\n",
    "    the average loudness is commonly referred to as the audio intensity. Guidance for this function \n",
    "    was taken from:\n",
    "    Harris et al. (2020)\n",
    "    and \n",
    "    AJonas Adler (2022)\n",
    "    '''\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    absolute_intensity_val  = np.abs(y) #not sure if np (absolute or abs) function should be used here\n",
    "    mean_intensity = np.mean(absolute_intensity_val)\n",
    "    return mean_intensity\n",
    "\n",
    "def process_directory(directory):\n",
    "    intensities = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.wav'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            intensity = calculate_mean_intensity(file_path)\n",
    "            intensities.append((file, intensity))\n",
    "    return intensities\n",
    "\n",
    "\n",
    "for utterance in utterances:\n",
    "    print(f\"running for utterance: '{utterance}'\")\n",
    "    all_data = {}\n",
    "\n",
    "    for accent, base_dir in base_directories.items():\n",
    "        directory = os.path.join(base_dir, utterance)\n",
    "        intensities = process_directory(directory)\n",
    "        df = pd.DataFrame(intensities, columns=['filename', 'intensity'])\n",
    "        all_data[accent] = df\n",
    "\n",
    "    # balancing the data between the accents, have some discrepency in some cases\n",
    "    min_count = min(len(all_data[accent]) for accent in all_data)\n",
    "    for accent in all_data:\n",
    "        if len(all_data[accent]) > min_count:\n",
    "            all_data[accent] = all_data[accent].sample(n=min_count, random_state=8)\n",
    "    \n",
    "    # mann whitney U test resutls and permutation results for each set of accents\n",
    "    print(\"mann whitney U and permutation results for utterance:\", utterance)\n",
    "    accents = list(all_data.keys())\n",
    "    for i in range(len(accents)):\n",
    "        for j in range(i+1, len(accents)):\n",
    "            accent_1, accent_2 = accents[i], accents[j]\n",
    "            data_1 = all_data[accent_1]['intensity']\n",
    "            data_2 = all_data[accent_2]['intensity']\n",
    "            stat, p_value, p_value_perm = perform_mann_whitney_and_permutation_test(data_1.values, data_2.values)\n",
    "            print(f\"between {accent_1} and {accent_2}: U={stat}, p-value={p_value}, permutation p-value={p_value_perm}\")\n",
    "    print(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacd7fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e4384f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791ac1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55df8e4b",
   "metadata": {},
   "source": [
    "<H2>Constanant Analysis<H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fef59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import scipy.stats as stats\n",
    "# if you are not running this script in a notebook environment, you may need to import\n",
    "# perform_mann_whitney_and_permutation_test from the pitch script.\n",
    "\n",
    "def get_energy_features(file_path):\n",
    "    '''\n",
    "    return the count of peaks and average energy of these peaks. This \n",
    "    code was guided by work completed in:\n",
    "    iranroman (2018)\n",
    "    and \n",
    "    Plotly Technologies Inc (2015)\n",
    "    '''\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    frame_length = 1024\n",
    "    hop_length = 512\n",
    "    energy = np.array([sum(abs(y[i:i+frame_length]**2)) for i in range(0, len(y), hop_length)])\n",
    "    peaks, _ = find_peaks(energy, height=np.mean(energy))\n",
    "    num_peaks = len(peaks)\n",
    "    mean_energy = np.mean(energy[peaks]) if peaks.size > 0 else 0\n",
    "    return {\"num_peaks\": num_peaks, \"mean_energy\": mean_energy}\n",
    "\n",
    "def analyse_directory(directory):\n",
    "    # iterate over the directory containing wav files and process each one\n",
    "    data = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.wav'):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            features = get_energy_features(file_path)\n",
    "            data.append(features)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# changing the utterance list and base directories since the focus is now on constonants\n",
    "utterances = ['d', 't']\n",
    "base_directories = {\n",
    "    'Dublin': '/home/garrett/Workspace/msc/speech_seg/dublin/features/Constant_Quality/',\n",
    "    'Liverpool':'/home/garrett/Workspace/msc/speech_seg/liverpool/features/Constant_Quality/',\n",
    "    'London': '/home/garrett/Workspace/msc/speech_seg/london/features/Constant_Quality/',\n",
    "}\n",
    "\n",
    "# this is the dataframe that will keep all the feature data\n",
    "accent_features_df = pd.DataFrame()\n",
    "\n",
    "# iterate through the utterances and process each one\n",
    "for utterance in utterances:\n",
    "    for accent, base_dir in base_directories.items():\n",
    "        directory = os.path.join(base_dir, utterance, utterance) \n",
    "        df = analyse_directory(directory)\n",
    "        df['Group'] = accent\n",
    "        df['Utterance'] = utterance\n",
    "        accent_features_df = pd.concat([accent_features_df, df])\n",
    "\n",
    "# run all various tests, these include:\n",
    "# shapiro wilk \n",
    "# levene test \n",
    "# mann whitney u and permutation test\n",
    "for utterance in utterances:\n",
    "    print(\" \")\n",
    "    print(f\"statistical test on utterance: '{utterance}'\")\n",
    "    for feature in ['num_peaks', 'mean_energy']:\n",
    "        print(\" \")\n",
    "        print(f\"feature: {feature}\")\n",
    "        #shapiro testing\n",
    "        for accent in base_directories.keys():\n",
    "            data = accent_features_df[(accent_features_df['Group'] == accent) & \n",
    "                                      (accent_features_df['Utterance'] == utterance)][feature]\n",
    "            _, p_value_shapiro = stats.shapiro(data)\n",
    "            print(f\"{accent} shapiro wilk p-value on {feature}: {p_value_shapiro}\")\n",
    "        \n",
    "        #levens testing\n",
    "        data_groups = [accent_features_df[(accent_features_df['Group'] == accent) & \n",
    "                                          (accent_features_df['Utterance'] == utterance)][feature] for accent in base_directories]\n",
    "        _, p_value_levene = stats.levene(*data_groups)\n",
    "        print(f\"levene test p-val for {feature}: {p_value_levene}\")\n",
    "\n",
    "        # mann whitney testsing\n",
    "        accents = list(base_directories.keys())\n",
    "        for i in range(len(accents)-1):\n",
    "            for x in range(i+1, len(accents)):\n",
    "                data_1 = accent_features_df[(accent_features_df['Group'] == accents[i]) & \n",
    "                                            (accent_features_df['Utterance'] == utterance)][feature]\n",
    "                data_2 = accent_features_df[(accent_features_df['Group'] == accents[x]) & \n",
    "                                            (accent_features_df['Utterance'] == utterance)][feature]\n",
    "                stat, p_value, p_value_perm = perform_mann_whitney_and_permutation_test(data_1, data_2)\n",
    "                print(f\"mann whitney u and permutation test between accents {accents[i]} and {accents[x]} for {feature}: U={stat}, p-value={p_value}, Permutation p-value={p_value_perm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b61231f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d91c6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f7eff44",
   "metadata": {},
   "source": [
    "<H2>Spectral Feature Analysis on Constonants<H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d0abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "# if you are not running this script in a notebook environment, you may need to import\n",
    "# perform_mann_whitney_and_permutation_test from the pitch script.\n",
    "\n",
    "def get_spectral_feature_data(file_path):\n",
    "    '''\n",
    "    this function is responsible for getting the spectral centroid, bandwidth\n",
    "    , flatness and rolloff. It follows methodology demonstrated in notebook found in :\n",
    "    imsparsh. (2015)\n",
    "    and \n",
    "    McFee et al. (2015) \n",
    "    '''\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    spectral_flatness = librosa.feature.spectral_flatness(y=y)\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    return np.mean(spectral_centroid), np.mean(spectral_bandwidth), np.mean(spectral_flatness), np.mean(spectral_rolloff)\n",
    "\n",
    "def process_directory(directory):\n",
    "    features = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.wav'):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            features.append(get_spectral_feature_data(file_path))\n",
    "    return features\n",
    "\n",
    "\n",
    "# this dataframe will hold all the accent feature data for analysis\n",
    "accent_features_df = pd.DataFrame()\n",
    "\n",
    "# iterate through utterances and get spectral features for each utterance in\n",
    "# the base directory\n",
    "for utterance in utterances:\n",
    "    for accent, base_dir in base_directories.items():\n",
    "        directory = os.path.join(base_dir, utterance, utterance) \n",
    "        features = process_directory(directory)\n",
    "        df = pd.DataFrame(features, columns=['spectral centroid', 'spectral bandwidth', \n",
    "                                             'spectral flatness', 'spectral roll-off'])\n",
    "        df['Accent'] = accent\n",
    "        df['Utterance'] = utterance\n",
    "        accent_features_df = pd.concat([accent_features_df, df])\n",
    "\n",
    "# stats output and mann whitney u and permutation testing\n",
    "for utterance in utterances:\n",
    "    print(\" \")\n",
    "    print(f\"stats output for utterance: '{utterance}'\")\n",
    "    for feature in ['spectral centroid', 'spectral bandwidth', 'spectral flatness', 'spectral roll-off']:\n",
    "        print(\" \")\n",
    "        print(f\"processing feature: {feature}\")\n",
    "        accents = accent_features_df['Accent'].unique()\n",
    "        for i in range(len(accents) - 1):\n",
    "            for x in range(i + 1, len(accents)):\n",
    "                accent_1 = accents[i]\n",
    "                accent_2 = accents[x]\n",
    "                accent_feature_data_1 = accent_features_df[(accent_features_df['Accent'] == accent_1) & \n",
    "                                         (accent_features_df['Utterance'] == utterance)][feature]\n",
    "                accent_feature_data_2 = accent_features_df[(accent_features_df['Accent'] == accent_2) & \n",
    "                                         (accent_features_df['Utterance'] == utterance)][feature]\n",
    "                stat, p_value, p_value_perm = perform_mann_whitney_and_permutation_test(\n",
    "                    accent_feature_data_1, accent_feature_data_2)\n",
    "                print(f\"mann whitney u and perm test on {accent_1} and {accent_2} with feature -{feature}: U={stat}, p-value={p_value}, Permutation p-value={p_value_perm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2012a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6ea9b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63dbfb29",
   "metadata": {},
   "source": [
    "<H2>Cluster Analysis<H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, adjusted_rand_score\n",
    "import seaborn as sns\n",
    "\n",
    "def mean_pitch(y, sr):\n",
    "    pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "    pitch = []\n",
    "    for t in range(pitches.shape[1]):\n",
    "        index = magnitudes[:, t].argmax()\n",
    "        if magnitudes[index, t] > 0:\n",
    "            pitch.append(pitches[index, t])\n",
    "\n",
    "    if len(pitch) == 0:\n",
    "        return 0 \n",
    "\n",
    "    return np.mean(pitch)\n",
    "\n",
    "def detect_sound_duration(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    hop_length = 512\n",
    "    frame_length = 1024\n",
    "    energy = np.array([\n",
    "        sum(abs(y[i:i+frame_length]**2))\n",
    "        for i in range(0, len(y), hop_length)\n",
    "    ])\n",
    "    \n",
    "    normalised_energy = energy / np.max(energy)\n",
    "    threshold = 0.01\n",
    "    onset_frames = np.where(normalised_energy > threshold)[0]\n",
    "    \n",
    "    if len(onset_frames) == 0:\n",
    "        return 0\n",
    "    \n",
    "    onset_time = librosa.frames_to_time(onset_frames[0], sr=sr, hop_length=hop_length)\n",
    "    offset_time = librosa.frames_to_time(onset_frames[-1], sr=sr, hop_length=hop_length)\n",
    "    \n",
    "    return offset_time - onset_time\n",
    "\n",
    "def calculate_mean_intensity(y):\n",
    "    return np.mean(np.abs(y))\n",
    "\n",
    "def trim_silence(audio, sr):\n",
    "    return audio[2*sr:-2*sr] \n",
    "\n",
    "\n",
    "def analyse_directory(directory):\n",
    "    data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.wav'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            y, sr = librosa.load(file_path)\n",
    "            audio_trimmed = trim_silence(y, sr)\n",
    "            pitch = mean_pitch(y, sr)\n",
    "            duration = detect_sound_duration(file_path)\n",
    "            intensity = calculate_mean_intensity(y)\n",
    "            formants = get_formants(audio_trimmed, sr)\n",
    "            data.append([filename, pitch, duration, intensity] + list(formants))\n",
    "    \n",
    "    columns = ['Filename', 'Pitch', 'Duration', 'Intensity', 'F1', 'F2', 'F3']\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "# back to using vowel utterances\n",
    "vowels = ['a', 'i', 'o', 'u']\n",
    "base_directories = {\n",
    "    'Dublin': '/home/garrett/Workspace/msc/speech_seg/dublin/features/Monophthongs/',\n",
    "    'Liverpool': '/home/garrett/Workspace/msc/speech_seg/liverpool/features/Monophthongs/',\n",
    "}\n",
    "group_to_label = {'Dublin': 0, 'Liverpool': 1}\n",
    "\n",
    "\n",
    "for vowel in vowels:\n",
    "    combined_df = pd.DataFrame()\n",
    "    for city, base_dir in base_directories.items():\n",
    "        directory = os.path.join(base_dir, vowel)\n",
    "        df = analyse_directory(directory)\n",
    "        df['Group'] = city\n",
    "        df['Vowel'] = vowel\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    \n",
    "    # balance the dataset since there is some imbalancing between the 2 groups\n",
    "    min_count = combined_df['Group'].value_counts().min()\n",
    "    balanced_df = pd.DataFrame()\n",
    "    for city in base_directories.keys():\n",
    "        city_df = combined_df[combined_df['Group'] == city]\n",
    "        balanced_df = pd.concat([balanced_df, city_df.sample(n=min_count, random_state=42)], ignore_index=True)\n",
    "    \n",
    "    balanced_df['TrueLabel'] = balanced_df['Group'].map(group_to_label)\n",
    "    \n",
    "    # features to use for the clustering model\n",
    "    features = ['Pitch', 'Duration', 'Intensity', 'F1']\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(balanced_df[features])\n",
    "    \n",
    "    # K-means clustering\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "    kmeans.fit(scaled_features)\n",
    "    balanced_df['Cluster'] = kmeans.labels_\n",
    "    \n",
    "    # PCA for visualization - There is no real benefit to using this other than educational\n",
    "    pca = PCA(n_components=2)\n",
    "    principalComponents = pca.fit_transform(scaled_features)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(kmeans.n_clusters):\n",
    "        plt.scatter(principalComponents[balanced_df['Cluster'] == i, 0], principalComponents[balanced_df['Cluster'] == i, 1], label=f'Cluster {i}')\n",
    "    plt.legend()\n",
    "    plt.title(f'PCA Cluster Visualization for Vowel: {vowel}')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.show()\n",
    "    \n",
    "    # create conf matrix and include the scores in outout\n",
    "    cm = confusion_matrix(balanced_df['TrueLabel'], balanced_df['Cluster'])\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=group_to_label.keys(), yticklabels=group_to_label.keys())\n",
    "    plt.title(f'confusion matrix on vowel: {vowel}')\n",
    "    plt.ylabel('true label')\n",
    "    plt.xlabel('cluster label')\n",
    "    plt.show()\n",
    "    \n",
    "    # show performance scoring\n",
    "    accuracy = accuracy_score(balanced_df['TrueLabel'], balanced_df['Cluster'])\n",
    "    precision = precision_score(balanced_df['TrueLabel'], balanced_df['Cluster'], zero_division=0)\n",
    "    recall = recall_score(balanced_df['TrueLabel'], balanced_df['Cluster'], zero_division=0)\n",
    "    f1 = f1_score(balanced_df['TrueLabel'], balanced_df['Cluster'], zero_division=0)\n",
    "    ari = adjusted_rand_score(balanced_df['TrueLabel'], balanced_df['Cluster'])\n",
    "    print(f\"vowel: {vowel}, accuracy: {accuracy}, precision: {precision}, recall: {recall}, F1 score: {f1}, adjusted rand index: {ari}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
